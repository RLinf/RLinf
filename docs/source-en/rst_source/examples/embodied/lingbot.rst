Lingbot-VLA Model Native Integration & Evaluation
==================================================

This document introduces how to integrate LingBot-VLA as a native plugin into the RLinf framework and perform end-to-end policy evaluation and RL fine-tuning within the RoboTwin 2.0 simulation environment. Unlike traditional WebSocket communication, this native integration thoroughly embeds LingBot-VLA into RLinf's Python memory space, achieving the highest efficiency in interaction and training.

The primary goal is to equip the model with the following capabilities:

* **Visual Understanding**: Process multi-view RGB images from robot cameras (e.g., head, wrist).
* **Language Understanding**: Comprehend and generalize from natural language task descriptions.
* **Action Generation**: Generate high-dimensional continuous action chunks auto-regressively via the large model foundation (based on Qwen2.5-VL).
* **Native Interaction**: Interact directly with the RoboTwin simulation environment at the Tensor level with zero latency within the RLinf framework.

Environment
-----------

**RoboTwin Environment**

* **Environment**: RoboTwin 2.0 physical simulation benchmark based on Sapien.
* **Task**: Command ALOHA (and other dual/single-arm robots) to complete complex household and manipulation skills (e.g., ``click_bell``, ``open_microwave``, ``stack_blocks_three``).
* **Observation**: RGB images collected from multi-camera perspectives.
* **Action Space**: 14-dimensional continuous actions (using dual-arm ALOHA as an example), including absolute poses (x, y, z, roll, pitch, yaw) and gripper states for both arms.

Task Description Format
-----------------------

LingBot-VLA directly utilizes the natural language task descriptions provided by the environment as text Prompts for the Vision-Language Model (VLM).

Data Structure
--------------

* **Images**: RGB images from the main (Head) view and left/right Wrist views.
* **Task Descriptions**: Natural language instructions (e.g., "click the bell").
* **Actions**: Action chunks of length 50 (configurable), executed via an open-loop/closed-loop strategy based on historical observations.

Dependency Installation
-----------------------

To achieve perfect compatibility between the higher version of Torch (2.8.0) and RLinf (Python 3.10), we have encapsulated the complex dependency isolation logic into the installation script. Please follow the steps below to build the hybrid environment.

1. Clone RLinf Repository
~~~~~~~~~~~~~~~~~~~~~~~~~

First, clone the RLinf repository and enter the directory:

.. code-block:: bash

    export WORK_DIR="/path/to/your/workspace"
    mkdir -p ${WORK_DIR} && cd ${WORK_DIR}

    git clone https://github.com/RLinf/RLinf.git
    cd RLinf
    export RLINF_PATH=$(pwd)

2. Install Dependencies
~~~~~~~~~~~~~~~~~~~~~~~

**Option 1: Docker Image**

Use the Docker image for RoboTwin-based embodied training:

.. code-block:: bash

    docker run -it --rm --gpus all \
      --shm-size 20g \
      --network host \
      --name rlinf \
      -v .:/workspace/RLinf \
      rlinf/rlinf:embodied-rlinf0.1-robotwin

Please switch to the corresponding virtual environment via the built-in `switch_env` utility in the image:

.. code-block:: bash

    source switch_env lingbotvla

**Option 2: Custom Environment**

Install dependencies directly in your environment. The script will automatically pull the LingBot source code into the `.venv/lingbot-vla` directory and resolve all high-risk dependency conflicts:

.. code-block:: bash

    bash requirements/install.sh embodied --model lingbot-vla --env robotwin --use-mirror --no-root
    source .venv/bin/activate

3. RoboTwin Environment Setup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since the built-in RLinf environment does not contain the complete RoboTwin source code, you need to manually pull the ``RLinf_support`` branch of RoboTwin (this branch already includes all necessary patches compatible with LingBot-VLA).

.. code-block:: bash

    cd ${RLINF_PATH}
    git clone https://github.com/RoboTwin-Platform/RoboTwin.git -b RLinf_support
    cd RoboTwin
    export ROBOTWIN_PATH=$(pwd)
    export HF_ENDPOINT=https://hf-mirror.com
    bash script/_download_assets.sh

Model Download
--------------

Before starting the evaluation or training, navigate to the automatically generated LingBot-VLA source directory and download the base weights and the Qwen foundation model from HuggingFace:

.. code-block:: bash

    # Enter the lingbot directory automatically generated by install.sh
    export LINGBOT_PATH="${RLINF_PATH}/.venv/lingbot-vla"
    cd ${LINGBOT_PATH}

    # Method 1: Using git clone
    git lfs install
    git clone https://huggingface.co/robbyant/lingbot-vla-4b
    git clone https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct

    # Method 2: Using huggingface-hub
    pip install huggingface-hub
    huggingface-cli download robbyant/lingbot-vla-4b --local-dir lingbot-vla-4b
    huggingface-cli download Qwen/Qwen2.5-VL-3B-Instruct --local-dir Qwen2.5-VL-3B-Instruct
    
    # Eliminate the nested folder trap generated by the download
    cd lingbot-vla-4b
    mv lingbot-vla-4b/* . 
    rmdir lingbot-vla-4b
    cd ..
    
Quick Start
-----------

Configuration File
~~~~~~~~~~~~~~~~~~

Prepare the RLinf evaluation configuration file for LingBot: ``examples/embodiment/config/robotwin_click_bell_eval_lingbot.yaml``

**Key Configuration Snippet:**

.. code-block:: yaml

    rollout:
      group_name: "RolloutGroup"
      backend: "huggingface"
      enable_offload: False
      pipeline_stage_num: 1
      model:
        model_type: "lingbot"
        model_path: /path/to/RLinf/.venv/lingbot-vla/lingbot-vla-4b  # Please replace with the actual absolute path
        tokenizer_path: /path/to/RLinf/.venv/lingbot-vla/Qwen2.5-VL-3B-Instruct
        action_dim: 14
        num_action_chunks: 50

    env:
      eval:
        total_num_envs: 16
        max_episode_steps: 300
        is_eval: True
        video_cfg:
          save_video: True
          video_base_dir: /path/to/RLinf/RoboTwin/eval_result/click_bell/policy.lingbot_wrapper/demo_clean

Evaluation
----------

Once the model configuration is complete, use the official RLinf evaluation script to boot up the Ray cluster for distributed parallel evaluation.
**Note:** To prevent CPU/GPU Out-Of-Memory (OOM) and timeout crashes caused by single-node rendering, it is highly recommended to limit the concurrent high-fidelity physical simulation environments (``total_num_envs``) to 4.

.. code-block:: bash

    cd ${RLINF_PATH}
    source .venv/bin/activate

    # 1. Start a clean Ray Cluster (Bind to local IP and custom port to prevent Dashboard crash conflicts)
    ray stop --force
    export RAY_TMPDIR=/tmp/ray_private_tmp
    ray start --head --node-ip-address=127.0.0.1 --dashboard-host=127.0.0.1 --dashboard-port=8277 --include-dashboard=True

    # 2. Declare business and offline environment variables
    unset RAY_ADDRESS
    export ROBOT_PLATFORM=ALOHA
    export ROBOTWIN_PATH=${ROBOTWIN_PATH}
    export LINGBOT_PATH="${RLINF_PATH}/.venv/lingbot-vla"
    export LINGBOT_VLA_PATH="${LINGBOT_PATH}"   # Point to the model/training root directory (adjust as needed)
    export PYTHONPATH=${RLINF_PATH}:${LINGBOT_PATH}:${ROBOTWIN_PATH}:$PYTHONPATH

    # [Optional] For offline compute nodes, force HuggingFace to bypass network requests
    export HF_DATASETS_OFFLINE=1
    export TRANSFORMERS_OFFLINE=1
    export HF_HUB_OFFLINE=1

    # [Mandatory] Hijack imageio to use the system-level complete ffmpeg (Fixes native missing libopenh264 encoder error)
    export IMAGEIO_FFMPEG_EXE=$(which ffmpeg)

    # 3. Fix the hardcoded ROBOTWIN_PATH in the execution script
    sed -i 's|export ROBOTWIN_PATH="/path/to/RoboTwin"|export ROBOTWIN_PATH=${ROBOTWIN_PATH}|g' examples/embodiment/eval_embodiment.sh

    # 4. Execute evaluation (replace ${LINGBOT_VLA_PATH} with the actual path or keep the setting above)
    bash examples/embodiment/eval_embodiment.sh robotwin_click_bell_eval_lingbot ALOHA \
        ++rollout.model.model_path="${LINGBOT_VLA_PATH}/output_mixed_5tasks_aloha/checkpoints/global_step_46400/hf_ckpt" \
        ++actor.model.model_path="${LINGBOT_VLA_PATH}/output_mixed_5tasks_aloha/checkpoints/global_step_46400/hf_ckpt" \
        ++rollout.model.tokenizer_path="${LINGBOT_VLA_PATH}/Qwen2.5-VL-3B-Instruct" \
        ++actor.model.tokenizer_path="${LINGBOT_VLA_PATH}/Qwen2.5-VL-3B-Instruct"

Visualization and Results
-------------------------

Upon completion of the tests, relevant success rate statistics, action logs, and rendered videos will be uniformly saved in the path specified by ``video_base_dir`` in the configuration file (e.g., ``RoboTwin/eval_result/click_bell/...``).

* **Video Records**: You can confirm whether LingBot-VLA's spatial positioning, gripper actions, and trajectory smoothness meet expectations via the generated ``.mp4`` recordings.
* **Success Rate**: The overall task success rate of the test seeds will be recorded in the output log files.