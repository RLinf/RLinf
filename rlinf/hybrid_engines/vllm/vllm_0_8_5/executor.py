# Copyright 2025 The RLinf Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import signal
import sys
import threading
import weakref
from typing import Dict, List, Optional

import psutil
from omegaconf import DictConfig
from vllm.config import VllmConfig
from vllm.distributed.device_communicators.shm_broadcast import Handle, MessageQueue
from vllm.executor.multiproc_worker_utils import (
    _add_prefix,
    set_multiprocessing_worker_envs,
)
from vllm.logger import init_logger
from vllm.utils import get_distributed_init_method, get_mp_context, get_open_port
from vllm.v1.executor.abstract import FailureCallback
from vllm.v1.executor.multiproc_executor import (
    MultiprocExecutor,
    UnreadyWorkerProcHandle,
    WorkerProc,
    WorkerProcHandle,
)
from vllm.worker.worker_base import WorkerWrapperBase

from rlinf.scheduler.manager.worker_manager import WorkerAddress
from rlinf.utils.placement import ModelParallelComponentPlacement

logger = init_logger(__name__)


class VLLMExecutor(MultiprocExecutor):
    def __init__(
        self,
        vllm_config: VllmConfig,
        rlinf_config: DictConfig,
        dp_rank: int,
        parent_address: WorkerAddress,
        placement: ModelParallelComponentPlacement,
    ):
        self.rlinf_config = rlinf_config
        self.parent_address = parent_address
        self.placement = placement
        self.dp_rank = dp_rank
        self.tp_size = vllm_config.parallel_config.tensor_parallel_size
        super().__init__(vllm_config)

    def _init_executor(self) -> None:
        """
        This function is copied and modified from
        vllm.v1.executor.multiproc_executor.MultiprocExecutor._init_executor.
        because it hardcodes the worker class which does not allow us to use
        our customized worker class, we changed it to use our customized
        worker class VLLMWorkerProc.
        """
        self._finalizer = weakref.finalize(self, self.shutdown)

        # The child processes will send SIGUSR1 when unrecoverable
        # errors happen.
        def sigusr1_handler(signum, frame):
            logger.fatal(
                "MulitprocExecutor got fatal signal from worker processes, "
                "shutting down. See stack trace above for root cause issue."
            )
            # Propagate error up to parent process.
            parent_process = psutil.Process().parent()
            parent_process.send_signal(signal.SIGUSR1)
            self.shutdown()

        signal.signal(signal.SIGUSR1, sigusr1_handler)

        self.is_failed = False
        self.shutdown_event = threading.Event()
        self.failure_callback: Optional[FailureCallback] = None

        self.world_size = self.parallel_config.world_size
        tensor_parallel_size = self.parallel_config.tensor_parallel_size
        assert self.world_size == tensor_parallel_size, (
            f"world_size ({self.world_size}) must be equal to the "
            f"tensor_parallel_size ({tensor_parallel_size}). "
            f"Pipeline parallelism is not yet implemented in v1"
        )

        # Set multiprocessing envs that are common to V0 and V1
        set_multiprocessing_worker_envs(self.parallel_config)

        # Multiprocessing-based executor does not support multi-node setting.
        # Since it only works for single node, we can use the loopback address
        # 127.0.0.1 for communication.
        distributed_init_method = get_distributed_init_method(
            "127.0.0.1", get_open_port()
        )

        # Initialize worker and set up message queues for SchedulerOutputs
        # and ModelRunnerOutputs
        self.rpc_broadcast_mq = MessageQueue(self.world_size, self.world_size)
        scheduler_output_handle = self.rpc_broadcast_mq.export_handle()

        # Create workers
        unready_workers: list[UnreadyWorkerProcHandle] = []
        # Set multiprocessing envs that are common to V0 and V1
        set_multiprocessing_worker_envs(self.parallel_config)

        # Multiprocessing-based executor does not support multi-node setting.
        # Since it only works for single node, we can use the loopback address
        # 127.0.0.1 for communication.
        distributed_init_method = get_distributed_init_method(
            "127.0.0.1", get_open_port()
        )

        # Initialize worker and set up message queues for SchedulerOutputs
        # and ModelRunnerOutputs
        self.rpc_broadcast_mq = MessageQueue(self.world_size, self.world_size)
        scheduler_output_handle = self.rpc_broadcast_mq.export_handle()

        # Create workers
        # Create workers
        unready_workers: list[UnreadyWorkerProcHandle] = []
        success = False
        try:
            for rank in range(self.world_size):
                worker = VLLMWorkerProc.make_worker_process(
                    local_rank=rank,
                    rank=rank,
                    distributed_init_method=distributed_init_method,
                    parent_address=self.parent_address,
                    placement=self.placement,
                    vllm_config=self.vllm_config,
                    rlinf_config=self.rlinf_config,
                    input_shm_handle=scheduler_output_handle,
                )
                unready_workers.append(worker)
            self.workers = VLLMWorkerProc.wait_for_ready(unready_workers)

            # Ensure message queues are ready. Will deadlock if re-ordered
            # Must be kept consistent with the WorkerProc
            self.rpc_broadcast_mq.wait_until_ready()
            for w in self.workers:
                w.worker_response_mq.wait_until_ready()
            self.start_worker_monitor()
            success = True
        finally:
            if not success:
                # Clean up the worker procs if there was a failure.
                self._ensure_worker_termination([w.proc for w in unready_workers])


class VLLMWorkerProc(WorkerProc):
    def __init__(
        self,
        vllm_config: VllmConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        input_shm_handle: Handle,
        parent_address: WorkerAddress,
        rlinf_config: DictConfig,
        placement: ModelParallelComponentPlacement,
    ):
        self.rank = rank  # global rank in 2D parallel (tp,pp)

        wrapper = WorkerWrapperBase(vllm_config=vllm_config, rpc_rank=rank)
        all_kwargs: List[Dict] = [
            {} for _ in range(vllm_config.parallel_config.world_size)
        ]
        all_kwargs[rank] = {
            # vllm former args
            "vllm_config": vllm_config,
            "local_rank": local_rank,
            "rank": rank,
            "distributed_init_method": distributed_init_method,
            "is_driver_worker": rank == 0,
            # rlinf specific args
            "parent_address": parent_address,
            "rlinf_config": rlinf_config,
            "placement": placement,
        }
        wrapper.init_worker(all_kwargs=all_kwargs)

        self.worker = wrapper.worker

        pid = os.getpid()
        _add_prefix(
            sys.stdout,
            f"VllmWorkerProc[dp_rank={self.worker.get_dp_rank()},tp_rank={self.rank}]",
            pid,
        )
        _add_prefix(
            sys.stderr,
            f"VllmWorkerProc[dp_rank={self.worker.get_dp_rank()},tp_rank={self.rank}]",
            pid,
        )
        # Initialize MessageQueue for receiving SchedulerOutput
        self.rpc_broadcast_mq = MessageQueue.create_from_handle(
            input_shm_handle, self.worker.rank
        )

        # Initializes a message queue for sending the model output
        self.worker_response_mq = MessageQueue(1, 1)

        self.worker.init_device()
        self.worker.load_model()
        # after load_model, we should save it's named_buffers to implement sync weight
        self.worker.use_sharded_weights()

    @staticmethod
    def make_worker_process(
        # vllm former args
        vllm_config: VllmConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        input_shm_handle: Handle,
        # rlinf specific args
        parent_address: WorkerAddress,
        rlinf_config: DictConfig,
        placement: ModelParallelComponentPlacement,
    ) -> WorkerProcHandle:
        """
        Note:
        This function is modified from vllm's raw implementation. because it in vllm hardcodes that
        it will launch `WorkerProc` which can't be selected, we can only copy and modify it here.
        """
        context = get_mp_context()
        reader, writer = context.Pipe(duplex=False)

        # ZMQ path for worker to send ready message and shm_broadcast handle
        # back to core process.
        process_kwargs = {
            # vllm former args
            "vllm_config": vllm_config,
            "local_rank": local_rank,
            "rank": rank,
            "distributed_init_method": distributed_init_method,
            "input_shm_handle": input_shm_handle,
            "ready_pipe": (reader, writer),
            # rlinf specific args
            "parent_address": parent_address,
            "rlinf_config": rlinf_config,
            "placement": placement,
        }

        proc = context.Process(
            target=VLLMWorkerProc.worker_main, kwargs=process_kwargs, daemon=True
        )

        proc.start()
        writer.close()
        return UnreadyWorkerProcHandle(proc, rank, reader)

    @staticmethod
    def worker_main(*args, **kwargs):
        """
        Note:
        This function is modified from vllm's raw implementation. because it in vllm hardcodes that
        it will launch `WorkerProc` which can't be selected, we can only copy and modify it here.
        """

        # Signal handler used for graceful termination.
        # SystemExit exception is only raised once to allow this and worker
        # processes to terminate without error
        shutdown_requested = False

        def signal_handler(signum, frame):
            nonlocal shutdown_requested
            if not shutdown_requested:
                shutdown_requested = True
                raise SystemExit()

        # Either SIGTERM or SIGINT will terminate the worker
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)

        worker = None
        # tuple[Connection, Connection]
        reader, ready_writer = kwargs.pop("ready_pipe")
        try:
            reader.close()
            worker = VLLMWorkerProc(*args, **kwargs)

            # Send READY once we know everything is loaded
            ready_writer.send(
                {
                    "status": WorkerProc.READY_STR,
                    "handle": worker.worker_response_mq.export_handle(),
                }
            )
            # Ensure message queues are ready. Will deadlock if re-ordered.
            # Must be kept consistent with the Executor
            worker.rpc_broadcast_mq.wait_until_ready()
            worker.worker_response_mq.wait_until_ready()
            ready_writer.close()
            ready_writer = None

            worker.worker_busy_loop()

        except Exception:
            # NOTE: if an Exception arises in busy_loop, we send
            # a FAILURE message over the MQ RPC to notify the Executor,
            # which triggers system shutdown.
            # TODO(rob): handle case where the MQ itself breaks.

            if ready_writer is not None:
                logger.exception("WorkerProc failed to start.")
            else:
                logger.exception("WorkerProc failed.")

            # The parent sends a SIGTERM to all worker processes if
            # any worker dies. Set this value so we don't re-throw
            # SystemExit() to avoid zmq exceptions in __del__.
            shutdown_requested = True
        finally:
            if ready_writer is not None:
                ready_writer.close()
            # Clean up once worker exits busy loop
            if worker is not None:
                worker.shutdown()
