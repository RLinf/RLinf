# Copyright 2025 The RLinf Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
import json
import os
import time
import typing
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Dict, List, Tuple

from omegaconf import DictConfig

from rlinf.data.io_struct import SeqGroupInfo
from rlinf.scheduler.worker.worker import Worker
from rlinf.utils.placement import ModelParallelComponentPlacement, PlacementMode

if typing.TYPE_CHECKING:
    from vllm.outputs import RequestOutput


COLOR_END = "\033[0m"


def green(text: str):
    return f"\033[32m{text}\033[0m"


@contextmanager
def sharp_cover(header_text: str, prelen: int = 30, color="\033[32m"):
    len(header_text)
    print("#" * prelen + f" {color}>>> {header_text}{COLOR_END} " + "#" * prelen)

    try:
        yield
    finally:
        print("#" * prelen + f" {color}>>> {header_text}{COLOR_END} " + "#" * prelen)


def print_vllm_outputs(outputs: List["RequestOutput"]):
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        generated_ids = output.outputs[0].token_ids
        print(
            f"{green('Prompt')}         : {prompt!r}",
            f"{green('Generated text')} : {generated_text!r}",
            f"{green('Generated ids')}  : {generated_ids}",
            sep="\n",
        )


def print_multi_outputs(resps_all: List[List["RequestOutput"]]):
    for i, resps in enumerate(resps_all):
        with sharp_cover(f"vllm dp {i}"):
            print_vllm_outputs(resps)


def print_sglang_outputs(prompts, outputs: List[Dict], tokenizer):
    output_ids = [output["output_ids"] for output in outputs]
    output_texts = tokenizer.batch_decode(output_ids)
    for p, t, ids in zip(prompts, output_texts, output_ids):
        print(
            f"{green('Prompt')}         : {p!r}",
            f"{green('Generated text')} : {t!r}",
            f"{green('Generated ids')}  : {ids}",
            sep="\n",
        )


def print_multi_sglang_outputs(prompts, outputs: List[List[Dict]], tokenizer):
    for i, resps in enumerate(outputs):
        with sharp_cover(f"sglang dp {i}"):
            print_sglang_outputs(prompts, resps, tokenizer)


class RankMapper:
    @classmethod
    def get_actor_rank_to_rollout_rank_map(
        cls,
        placement: ModelParallelComponentPlacement,
    ) -> Dict[int, List[Tuple[int, int]]]:
        return cls._get_rank_mapper(
            placement.placement_mode
        ).get_actor_rank_to_rollout_rank_map(
            placement.actor_tp_size,
            placement.actor_pp_size,
            placement.actor_world_size,
            placement.rollout_tp_size,
            placement.rollout_world_size,
        )

    @classmethod
    def get_rollout_rank_to_actor_rank_map(
        cls, placement: ModelParallelComponentPlacement
    ) -> Dict[Tuple[int, int], int]:
        return cls._get_rank_mapper(
            placement.placement_mode
        ).get_rollout_rank_to_actor_rank_map(
            placement.actor_tp_size,
            placement.actor_pp_size,
            placement.actor_world_size,
            placement.rollout_tp_size,
            placement.rollout_world_size,
        )

    @staticmethod
    def _get_rank_mapper(
        placement_mode: PlacementMode,
    ):
        """
        Get the rank mapper class based on the mode.
        """
        if placement_mode == PlacementMode.COLLOCATED:
            return CollocateRankMapper
        elif placement_mode in [PlacementMode.DISAGGREGATED, PlacementMode.AUTO]:
            return DisaggRankMapper
        else:
            raise ValueError(f"Unsupported mode: {placement_mode}.")


class CollocateRankMapper(RankMapper):
    @classmethod
    def get_actor_rank_to_rollout_rank_map(
        cls,
        actor_tp_size: int,
        actor_pp_size: int,
        actor_world_size: int,
        rollout_tp_size: int,
        rollout_world_size: int,
    ) -> Dict[int, Tuple[int, int]]:
        """
        Get the global mapping from actor 1D rank to rollout 2D rank as dict.
        """
        # rank -> (dp, tp)
        if actor_tp_size == 1:
            return {
                rank: (rank // rollout_tp_size, rank % rollout_tp_size)
                for rank in range(actor_world_size)
            }
        rank_map = {}
        for actor_rank in range(actor_world_size):
            rank_map[actor_rank] = cls._get_actor_rank_to_rollout_rank(
                actor_rank,
                actor_tp_size,
                rollout_tp_size,
            )
        return rank_map

    @classmethod
    def get_rollout_rank_to_actor_rank_map(
        cls,
        actor_tp_size: int,
        actor_pp_size: int,
        actor_world_size: int,
        rollout_tp_size: int,
        rollout_world_size: int,
    ):
        """
        Get the global mapping from rollout 2D rank to actor 1D rank as dict.
        """
        rank_map = cls.get_actor_rank_to_rollout_rank_map(
            actor_tp_size,
            actor_pp_size,
            actor_world_size,
            rollout_tp_size,
            rollout_world_size,
        )
        return {v: k for k, v in rank_map.items()}

    @staticmethod
    def _get_actor_rank_to_rollout_rank(
        actor_rank: int,
        actor_tp_size: int,
        rollout_tp_size: int,
    ):
        """
        Get the mapping from actor 1D rank to rollout 2D rank.
        """
        num_rollout_dp_ranks_per_actor_tp_group = actor_tp_size // rollout_tp_size

        actor_tp_rank = actor_rank % actor_tp_size

        actor_tp_group_id = actor_rank // actor_tp_size
        rollout_start_dp_rank = (
            actor_tp_group_id * num_rollout_dp_ranks_per_actor_tp_group
        )

        weight_dst_dp_rank_in_rollout = (
            rollout_start_dp_rank
            + actor_tp_rank % num_rollout_dp_ranks_per_actor_tp_group
        )

        weight_dst_tp_rank_in_rollout = (
            actor_tp_rank // num_rollout_dp_ranks_per_actor_tp_group
        )

        return (weight_dst_dp_rank_in_rollout, weight_dst_tp_rank_in_rollout)


class DisaggRankMapper(RankMapper):
    """
    A mapper for disaggregated ranks.
    This is used to map the disaggregated ranks to the actor ranks.

    Assume that actor_tp_size = n * rollout_tp_size
    """

    @classmethod
    def get_actor_rank_to_rollout_rank_map(
        cls,
        actor_tp_size: int,
        actor_pp_size: int,
        actor_world_size: int,
        rollout_tp_size: int,
        rollout_world_size: int,
    ) -> Dict[int, List[Tuple[int, int]]]:
        """
        Only ranks in dp=0 actor dp group will send weights to rollout LLM.
        """
        actor_model_parallel_size = actor_tp_size * actor_pp_size
        assert (
            rollout_world_size >= actor_model_parallel_size
            and rollout_world_size % actor_model_parallel_size == 0
        ), (
            f"rollout_world_size ({rollout_world_size}) should be a multiple of actor_model_parallel_size ({actor_model_parallel_size})"
        )

        num_dp_ranks_per_actor_dp_group = (
            rollout_world_size // actor_model_parallel_size
        )
        stride = actor_model_parallel_size // rollout_tp_size

        rank_map = {}
        for actor_rank in range(actor_world_size):
            if actor_rank >= actor_model_parallel_size:
                # dp_rank > 0 will not send weight to any rollout rank
                rank_map[actor_rank] = []
                continue
            gen_dp, gen_tp = cls._get_actor_rank_to_rollout_rank(
                actor_rank,
                actor_tp_size,
                rollout_tp_size,
            )
            rank_map[actor_rank] = [
                (gen_dp + i * stride, gen_tp)
                for i in range(num_dp_ranks_per_actor_dp_group)
            ]

        return rank_map

    @classmethod
    def get_rollout_rank_to_actor_rank_map(
        cls,
        actor_tp_size: int,
        actor_pp_size: int,
        actor_world_size: int,
        rollout_tp_size: int,
        rollout_world_size: int,
    ) -> Dict[Tuple[int, int], int]:
        rank_map = cls.get_actor_rank_to_rollout_rank_map(
            actor_tp_size,
            actor_pp_size,
            actor_world_size,
            rollout_tp_size,
            rollout_world_size,
        )
        result_map = {}
        for actor_rank, rollout_2d_ranks in rank_map.items():
            for rollout_2d_rank in rollout_2d_ranks:
                result_map[rollout_2d_rank] = actor_rank
        return result_map

    @staticmethod
    def _get_actor_rank_to_rollout_rank(
        actor_rank: int,
        actor_tp_size: int,
        rollout_tp_size: int,
    ) -> Tuple[int, int]:
        assert actor_tp_size % rollout_tp_size == 0, (
            "actor_tp_size must be a multiple of rollout_tp_size"
        )

        num_rollout_dp_ranks_per_actor_tp_group = actor_tp_size // rollout_tp_size
        actor_tp_rank = actor_rank % actor_tp_size
        corresponding_rollout_dp_rank = (
            actor_tp_rank % num_rollout_dp_ranks_per_actor_tp_group
        )
        corresponding_rollout_tp_rank = (
            actor_tp_rank // num_rollout_dp_ranks_per_actor_tp_group
        )

        return (corresponding_rollout_dp_rank, corresponding_rollout_tp_rank)


SUPPORTED_LLM_ROLLOUT_BACKENDS = ["vllm", "sglang"]


def get_rollout_backend_worker(
    cfg: DictConfig, placement: ModelParallelComponentPlacement
) -> Worker:
    rollout_backend = cfg.rollout.get("rollout_backend", None)
    if rollout_backend is None:
        raise ValueError(
            f"rollout_backend must be specified in the config. Support {', '.join(SUPPORTED_LLM_ROLLOUT_BACKENDS)}."
        )
    if rollout_backend not in SUPPORTED_LLM_ROLLOUT_BACKENDS:
        raise ValueError(
            f"rollout_backend {rollout_backend} is not supported. Support {', '.join(SUPPORTED_LLM_ROLLOUT_BACKENDS)}."
        )

    if rollout_backend == "vllm":
        from rlinf.workers.rollout.vllm.vllm_worker import VLLMWorker

        if (
            placement.placement_mode == PlacementMode.COLLOCATED
            or placement.placement_mode == PlacementMode.DISAGGREGATED
        ):
            return VLLMWorker
        else:
            raise ValueError(f"Unsupported placement mode: {placement.placement_mode}")
    elif rollout_backend == "sglang":
        from rlinf.workers.rollout.sglang.sglang_worker import SGLangWorker

        return SGLangWorker


class RunningStatusManager:
    def __init__(self):
        self._running_seq_group: Dict[SeqGroupInfo, asyncio.Task] = {}
        self._aborted_seq_group: List[SeqGroupInfo] = []
        # SeqGroupInfo that have been completed and sent to actor/inference
        # only retained for debugging
        self._done_seq_group: List[SeqGroupInfo] = []

        # asyncio Events
        # set by scheduler coroutine to prevent rollout coroutine from exiting before potential migrations
        self.exit_rollout_iter = asyncio.Event()

    def add_task(self, seq_group: SeqGroupInfo, task: asyncio.Task):
        assert seq_group not in self._running_seq_group, (
            f"Task for sequence group {seq_group.id} is already running."
        )
        self._running_seq_group[seq_group] = task

    def mark_done(self, seq_group: SeqGroupInfo):
        assert seq_group in self._running_seq_group, (
            f"Task for SeqGroup {seq_group.id} not found. "
            "Check whether it has been added correctly or already marked done."
        )
        assert seq_group not in self._done_seq_group
        self._running_seq_group.pop(seq_group)
        self._done_seq_group.append(seq_group)

    def mark_aborted(self, seq_group: SeqGroupInfo):
        assert seq_group in self._running_seq_group, (
            f"Task for SeqGroup {seq_group.id} not found. "
            "Check whether it has been added correctly or already marked aborted."
        )
        assert seq_group not in self._aborted_seq_group
        self._running_seq_group.pop(seq_group)
        self._aborted_seq_group.append(seq_group)

    async def wait_notification(self):
        """
        Wait until the scheduler notifies that it is safe to continue.
        This is used to prevent the rollout coroutine from exiting before potential migrations.
        """
        await self.exit_rollout_iter.wait()
        self.exit_rollout_iter.clear()

    def notify(self):
        """
        Call by scheduler to notify the rollout to continue.
        This is used to prevent the rollout coroutine from exiting before potential migrations.
        """
        self.exit_rollout_iter.set()

    def clear(self):
        self._running_seq_group.clear()
        self._aborted_seq_group.clear()
        self._done_seq_group.clear()
        self.exit_rollout_iter.clear()

    def empty(self) -> bool:
        return len(self._running_seq_group) == 0 and len(self._done_seq_group) == 0

    def get_running_seq_groups(self) -> list[SeqGroupInfo]:
        return list(self._running_seq_group.keys())

    def get_done_seq_groups(self) -> list[SeqGroupInfo]:
        return self._done_seq_group

    def get_aborted_seq_groups(self) -> list[SeqGroupInfo]:
        return self._aborted_seq_group

    def get_running_tasks(self) -> list[asyncio.Task]:
        return list(self._running_seq_group.values())

    @property
    def num_seq_group_running(self) -> int:
        return len(self._running_seq_group)

    @property
    def num_seq_group_done(self) -> int:
        return len(self._done_seq_group)

    @property
    def num_seq_group_aborted(self) -> int:
        return len(self._aborted_seq_group)

    @property
    def num_seq_group(self) -> int:
        return (
            self.num_seq_group_running
            + self.num_seq_group_done
            + self.num_seq_group_aborted
        )

    @property
    def num_seq_running(self) -> int:
        return sum(sg.num_running for sg in self.get_running_seq_groups())

    @property
    def num_seq_returned(self) -> int:
        return self.num_seq - self.num_seq_running

    @property
    def num_seq(self) -> int:
        return sum(sg.group_size for sg in self.get_running_seq_groups()) + sum(
            sg.group_size for sg in self.get_done_seq_groups()
        )


@dataclass
class RolloutEngineStats:
    num_running_reqs: int = 0
    max_running_reqs: int = 0
    num_used_tokens: int = 0
    max_total_num_tokens: int = 0
    token_usage: float = 0.0
    gen_throughput: float = 0.0
    num_queue_reqs: int = 0


class MetaInfoStatsCollector:
    """Collector for SGLang meta_info statistics

    This collector is only initialized when enabled via configuration.
    Add the following parameters to your generation config section:

    generation:
      collect_meta_stats: true  # Enable meta_info statistics collection
      meta_stats_file: "custom_meta_stats.jsonl"  # Optional: custom output file
      async_meta_stats_file: "custom_async_meta_stats.jsonl"  # Optional: custom async output file
      schedule_meta_stats_file: "custom_schedule_meta_stats.jsonl"  # Optional: custom schedule output file
    """

    def __init__(self, output_file: str):
        self.output_file = output_file
        self.stats_buffer = []
        self.buffer_size = 100  # Write to file every 100 records

        # Ensure output directory exists
        os.makedirs(
            os.path.dirname(self.output_file)
            if os.path.dirname(self.output_file)
            else ".",
            exist_ok=True,
        )

        # Initialize file with header if it doesn't exist
        if not os.path.exists(self.output_file):
            with open(self.output_file, "w") as f:
                f.write("")  # Create empty file

    def collect_batch_stats(self, outputs: List[Dict], batch_id: int) -> None:
        """Collect statistics from a batch of SGLang outputs

        Args:
            outputs: List of SGLang output dictionaries
            batch_id: Unique identifier for this batch
        """
        current_time = time.time()

        for req_idx, output in enumerate(outputs):
            try:
                # Extract meta_info
                meta_info = output.get("meta_info", {})

                # Extract the specific metrics you requested
                stats_record = {
                    "timestamp": current_time,
                    "batch_id": batch_id,
                    "request_id": f"batch_{batch_id}_req_{req_idx}",
                    "prompt_tokens": meta_info.get("prompt_tokens", None),
                    "completion_tokens": meta_info.get("completion_tokens", None),
                    "e2e_latency": meta_info.get("e2e_latency", None),
                    "ttft": meta_info.get("ttft", None),
                    # Additional useful meta_info fields (if available)
                    "finish_reason": meta_info.get("finish_reason", {}).get(
                        "type", None
                    ),
                    "total_tokens": (
                        meta_info.get("prompt_tokens", 0)
                        + meta_info.get("completion_tokens", 0)
                    )
                    if meta_info.get("prompt_tokens") is not None
                    and meta_info.get("completion_tokens") is not None
                    else None,
                    # Add any other meta_info fields that might be useful
                    "meta_info_keys": list(
                        meta_info.keys()
                    ),  # For debugging/inspection
                }

                self.stats_buffer.append(stats_record)

            except Exception as e:
                # Log error but continue processing
                error_record = {
                    "timestamp": current_time,
                    "batch_id": batch_id,
                    "request_id": f"batch_{batch_id}_req_{req_idx}",
                    "error": str(e),
                    "output_keys": list(output.keys())
                    if isinstance(output, dict)
                    else "not_dict",
                }
                self.stats_buffer.append(error_record)

        # Write to file if buffer is full
        if len(self.stats_buffer) >= self.buffer_size:
            self._flush_to_file()

    def _flush_to_file(self) -> None:
        """Write buffered statistics to file"""
        if not self.stats_buffer:
            return

        with open(self.output_file, "a") as f:
            for record in self.stats_buffer:
                f.write(json.dumps(record) + "\n")

        print(f"Written {len(self.stats_buffer)} records to {self.output_file}")
        self.stats_buffer = []

    def finalize(self) -> None:
        """Flush any remaining data and close"""
        self._flush_to_file()
        print(f"Finalized stats collection. Data saved to {self.output_file}")
